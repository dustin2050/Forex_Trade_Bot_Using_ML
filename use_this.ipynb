{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/data\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks/Forex_Trade_Bot_Using_ML/EURUSD.csv /content/data"
      ],
      "metadata": {
        "id": "iF4vqZU8MjJS",
        "outputId": "8e766d03-d6e1-425e-e231-bfb3105cc070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/content/data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "he9gxgVTKk-d",
        "outputId": "46396dbf-7a84-43f0-9fe2-6376f8de37ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'EURUSD.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c54b846a46b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0msignals_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EURUSD.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Preprocessing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EURUSD.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_sma(df, period, column=\"close\"):\n",
        "    return df[column].rolling(window=period).mean()\n",
        "\n",
        "def calculate_ema(df, period, column=\"close\"):\n",
        "    return df[column].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "def calculate_atr(df, period=14):\n",
        "    high_low = df['high'] - df['low']\n",
        "    high_close = np.abs(df['high'] - df['close'].shift())\n",
        "    low_close = np.abs(df['low'] - df['close'].shift())\n",
        "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    atr = true_range.rolling(window=period).mean()\n",
        "    return atr\n",
        "\n",
        "def calculate_bbands(df, period=20):\n",
        "    sma = calculate_sma(df, period)\n",
        "    stddev = df['close'].rolling(window=period).std()\n",
        "    upper_band = sma + (2 * stddev)\n",
        "    lower_band = sma - (2 * stddev)\n",
        "    return pd.DataFrame({\"BB_MIDDLE\": sma, \"BB_UPPER\": upper_band, \"BB_LOWER\": lower_band})\n",
        "\n",
        "def calculate_macd(df, fast_period=12, slow_period=26, signal_period=9):\n",
        "    fast_ema = calculate_ema(df, fast_period)\n",
        "    slow_ema = calculate_ema(df, slow_period)\n",
        "    macd = fast_ema - slow_ema\n",
        "    signal_line = macd.ewm(span=signal_period, adjust=False).mean()\n",
        "    return pd.DataFrame({\"MACD\": macd, \"SIGNAL_LINE\": signal_line})\n",
        "\n",
        "def calculate_rsi(df, period=14, column=\"close\"):\n",
        "    delta = df[column].diff(1)\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=period).mean()\n",
        "    avg_loss = loss.rolling(window=period).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "signals_df = pd.read_csv(\"EURUSD.csv\")\n",
        "\n",
        "# Preprocessing the data\n",
        "signals_df = signals_df[signals_df['volume'] != 0]\n",
        "signals_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "longest_MA_window = 200\n",
        "signals_df[\"9EMA\"] = calculate_ema(signals_df, 9).fillna(method='bfill')\n",
        "signals_df[\"20EMA\"] = calculate_ema(signals_df, 20).fillna(method='bfill')\n",
        "signals_df[\"50EMA\"] = calculate_ema(signals_df, 50).fillna(method='bfill')\n",
        "signals_df[\"200SMA\"] = calculate_sma(signals_df, longest_MA_window).fillna(method='bfill')\n",
        "\n",
        "# Setup Indicators\n",
        "signals_df[\"ATR\"] = calculate_atr(signals_df).fillna(method='bfill')\n",
        "bbands_df = calculate_bbands(signals_df)\n",
        "macd_df = calculate_macd(signals_df)\n",
        "signals_df[\"RSI\"] = calculate_rsi(signals_df).fillna(method='bfill')\n",
        "\n",
        "# Merge technical indicators\n",
        "bbands_df = pd.concat([bbands_df, macd_df], axis=1)\n",
        "signals_df = pd.concat([signals_df, bbands_df], axis=1)\n",
        "signals_df.drop(columns=\"SIGNAL_LINE\", inplace=True)\n",
        "# Create Bollinger Bands signals\n",
        "signals_df[\"Bollinger_Bands_Below_Lower_BB\"] = np.where(signals_df[\"close\"] < signals_df[\"BB_LOWER\"], 1, 0)\n",
        "signals_df[\"Bollinger_Bands_Above_Upper_BB\"] = np.where(signals_df[\"close\"] > signals_df[\"BB_UPPER\"], 1, 0)\n",
        "\n",
        "# Generate the 9EMA/20EMA crossover signals\n",
        "signals_df['9EMA_above_20EMA'] = np.where(signals_df['9EMA'] > signals_df['20EMA'], 1, 0)\n",
        "signals_df['9EMA_cross_20EMA'] = signals_df['9EMA_above_20EMA'].diff().fillna(0)\n",
        "\n",
        "# Generate the 50EMA/200SMA crossover signals\n",
        "signals_df['50EMA_above_200SMA'] = np.where(signals_df['50EMA'] > signals_df['200SMA'], 1, 0)\n",
        "signals_df['50EMA_cross_200SMA'] = signals_df['50EMA_above_200SMA'].diff().fillna(0)\n",
        "\n",
        "# --------------- Integrating Support and Resistance Signals with Engulfing/Star Patterns ---\n",
        "\n",
        "\n",
        "# Support and Resistance Functions\n",
        "def support(df, l, n1, n2):\n",
        "    for i in range(l - n1 + 1, l + 1):\n",
        "        if df['low'][i] > df['low'][i - 1]:  #If all lows in the past are higher and those in the future are lower, it's a support level.\n",
        "            return 0\n",
        "    for i in range(l + 1, l + n2 + 1):\n",
        "        if df['low'][i] < df['low'][i - 1]:\n",
        "            return 0\n",
        "    return 1\n",
        "\n",
        "def resistance(df, l, n1, n2):\n",
        "    for i in range(l - n1 + 1, l + 1):\n",
        "        if df['high'][i] < df['high'][i - 1]:\n",
        "            return 0\n",
        "    for i in range(l + 1, l + n2 + 1):\n",
        "        if df['high'][i] > df['high'][i - 1]:\n",
        "            return 0\n",
        "    return 1\n",
        "\n",
        "# Engulfing & Star Pattern Functions\n",
        "def isEngulfing(row, open_, close_):\n",
        "    bodydiffmin = 0.002\n",
        "    bodydiff = abs(open_[row] - close_[row])\n",
        "    prev_bodydiff = abs(open_[row - 1] - close_[row - 1])\n",
        "\n",
        "    if bodydiff < 0.000001:\n",
        "        bodydiff = 0.000001\n",
        "\n",
        "    if bodydiff > bodydiffmin and prev_bodydiff > bodydiffmin and open_[row - 1] < close_[row - 1] and open_[row] > close_[row]:\n",
        "        return 1  # Bearish Engulfing\n",
        "    elif bodydiff > bodydiffmin and prev_bodydiff > bodydiffmin and open_[row - 1] > close_[row - 1] and open_[row] < close_[row]:\n",
        "        return 2  # Bullish Engulfing\n",
        "    return 0\n",
        "\n",
        "def isStar(l, open_, close_, high, low):\n",
        "    bodydiffmin = 0.0020\n",
        "    bodydiff = abs(open_[l] - close_[l])\n",
        "    highdiff = high[l] - max(open_[l], close_[l])\n",
        "    lowdiff = min(open_[l], close_[l]) - low[l]\n",
        "\n",
        "    if bodydiff < 0.000001:\n",
        "        bodydiff = 0.000001\n",
        "\n",
        "    ratio1 = highdiff / bodydiff\n",
        "    ratio2 = lowdiff / bodydiff\n",
        "\n",
        "    if ratio1 > 1 and lowdiff < 0.2 * highdiff and bodydiff > bodydiffmin:\n",
        "        return 1  # Bearish Star\n",
        "    elif ratio2 > 1 and highdiff < 0.2 * lowdiff and bodydiff > bodydiffmin:\n",
        "        return 2  # Bullish Star\n",
        "    return 0\n",
        "\n",
        "# Signal Generation Function for Support/Resistance and Patterns\n",
        "def generate_support_resistance_signals(df, n1=2, n2=2, backCandles=30, lim=150e-5):\n",
        "    signal = [0] * len(df)\n",
        "    open_ = df['open']\n",
        "    close_ = df['close']\n",
        "    high = df['high']\n",
        "    low = df['low']\n",
        "\n",
        "    for row in range(backCandles, len(df) - n2):\n",
        "        ss = []  # Support levels\n",
        "        rr = []  # Resistance levels\n",
        "        for subrow in range(row - backCandles + n1, row + 1):\n",
        "            if support(df, subrow, n1, n2):\n",
        "                ss.append(low[subrow])\n",
        "            if resistance(df, subrow, n1, n2):\n",
        "                rr.append(high[subrow])\n",
        "\n",
        "        # Generate signals based on Engulfing and Star patterns\n",
        "        engulfing_pattern = isEngulfing(row, open_, close_)\n",
        "        star_pattern = isStar(row, open_, close_, high, low)\n",
        "\n",
        "        if ((engulfing_pattern == 1 or star_pattern == 1) and closeResistance(df, row, rr, lim)):\n",
        "            signal[row] = 1  # Sell Signal\n",
        "        elif ((engulfing_pattern == 2 or star_pattern == 2) and closeSupport(df, row, ss, lim)):\n",
        "            signal[row] = 2  # Buy Signal\n",
        "        else:\n",
        "            signal[row] = 0  # No Signal\n",
        "\n",
        "    df['Support_Resistance_Signal'] = signal\n",
        "    return df\n",
        "\n",
        "def closeResistance(df, l, levels, lim):\n",
        "    if len(levels) == 0:\n",
        "        return 0\n",
        "    return abs(df['high'][l] - min(levels, key=lambda x: abs(x - df['high'][l]))) <= lim\n",
        "\n",
        "def closeSupport(df, l, levels, lim):\n",
        "    if len(levels) == 0:\n",
        "        return 0\n",
        "    return abs(df['low'][l] - min(levels, key=lambda x: abs(x - df['low'][l]))) <= lim\n",
        "\n",
        "# Generate Support and Resistance Signals\n",
        "signals_df = generate_support_resistance_signals(signals_df)\n",
        "\n",
        "# Combine with existing signals\n",
        "# Create a new combined signal column\n",
        "signals_df['Combined_Signal'] = signals_df[['9EMA_cross_20EMA', '50EMA_cross_200SMA', 'Support_Resistance_Signal']].max(axis=1)\n",
        "\n",
        "# Proceed with the rest of the logic (Exit signals, news data integration, model training, etc.)\n",
        "\n",
        "# Initialize \"Exit\" and \"Exit Price\" columns\n",
        "signals_df[\"Exit Price\"] = np.nan\n",
        "signals_df[\"Exit\"] = np.nan\n",
        "\n",
        "# Exit signal (target labels for ML)\n",
        "num_rows_in_df = signals_df.shape[0]\n",
        "reward = 3\n",
        "risk = 1\n",
        "\n",
        "# Loop through and calculate exit price and signals\n",
        "for j in range(longest_MA_window, num_rows_in_df):\n",
        "    entry = signals_df[\"close\"].iloc[j]\n",
        "    atr = signals_df[\"ATR\"].iloc[j]\n",
        "    stop = entry - (risk * atr)\n",
        "    target = entry + (reward * atr)\n",
        "    for k in range(j + 1, num_rows_in_df):\n",
        "        curr_low = signals_df[\"low\"].iloc[k]\n",
        "        curr_high = signals_df[\"high\"].iloc[k]\n",
        "        if curr_low <= stop:\n",
        "            signals_df.at[j, \"Exit Price\"] = stop\n",
        "            signals_df.at[j, \"Exit\"] = -1\n",
        "            break\n",
        "        elif curr_high >= target:\n",
        "            signals_df.at[j, \"Exit Price\"] = target\n",
        "            signals_df.at[j, \"Exit\"] = 1\n",
        "            break\n",
        "\n",
        "# Drop rows that contain NaN values (from earlier calculations)\n",
        "signals_df = signals_df.dropna(subset=[\"Exit\"])\n",
        "\n",
        "# Map Exit values from [-1, 1] to [0, 1]\n",
        "# Map Exit values from [-1, 1, 2] to [0, 1, 1]\n",
        "signals_df[\"Exit\"] = signals_df[\"Exit\"].map({-1: 0, 1: 1, 2: 1})\n",
        "\n",
        "# Selecting features and target, adding Support/Resistance and Patterns to feature set\n",
        "X = signals_df[['9EMA', '20EMA', '50EMA', '200SMA', 'ATR', 'RSI', 'BB_UPPER', 'BB_MIDDLE', 'BB_LOWER', 'MACD','Bollinger_Bands_Below_Lower_BB','Bollinger_Bands_Above_Upper_BB','9EMA_above_20EMA','9EMA_cross_20EMA','50EMA_above_200SMA','50EMA_cross_200SMA']]\n",
        "y = signals_df['Exit']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost Model\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_test_pred = xgb_model.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# 1. Classification reports for training and test sets\n",
        "print(\"Classification Report for Training Data:\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "print(\"\\nClassification Report for Test Data:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# Define model directory and ensure it exists\n",
        "model_dir = os.path.join(os.getcwd(), \"model\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save the trained model\n",
        "model_path = os.path.join(model_dir, \"xgb_model2.pkl\")\n",
        "joblib.dump(xgb_model, model_path)\n",
        "\n",
        "print(f\"Model saved at {model_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hY5L5KX4Mehz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCsO1sfMKk-e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}